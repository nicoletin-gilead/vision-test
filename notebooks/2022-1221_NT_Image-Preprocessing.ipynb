{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Laplacian w CLAHE mods\n",
    "  \n",
    "Laplacian and FFT seems to still be the sota method for blur and sharpness   quantification   \n",
    "literature review on methods : https://www.researchgate.net/publication/234073157_Analysis_of_focus_measure_operators_in_shape-from-focus   \n",
    "\n",
    "   \n",
    "     \n",
    "## Thoughts (edit): \n",
    "1.   Use CLAHE to reduce noise-amplification, reduce brightness, from wet areasm and enhance image feeatures.      \n",
    "1b.  experiment with `tileGridSize` and `clipLimit`   \n",
    "1c.  also experiment with contrast limiting? (prelim results don't look promising)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"...\"\n",
    "# take 10 images\n",
    "file_names = os.listdir(root)[:10]\n",
    "images = []\n",
    "for name in file_names: images += [cv.imread(root + name)]\n",
    "# images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions: Image Display (edit)\n",
    "use `preview(img, width = 200)` :. `preview(image)` to display images in smaller cell    \n",
    "`preview` combines `cv2_imshow`, `.copy()`, and `.resize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- HELPER FUNCTIONS: IMAGE DISPLAY\n",
    "\n",
    "# shrink display size without altering original image\n",
    "def shrink(img, width=None, height=None, inter=cv.INTER_AREA):\n",
    "  # set width OR height pixels\n",
    "  image = img.copy()\n",
    "  dim = None\n",
    "  (h, w) = image.shape[:2]\n",
    "  \n",
    "  if width and height: return image\n",
    "  if height:\n",
    "      r = height / float(h)\n",
    "      dim = (int(w * r), height)\n",
    "  else:\n",
    "      r = width / float(w)\n",
    "      dim = (width, int(h * r))\n",
    "  return cv.resize(image, dim, interpolation=inter)\n",
    "\n",
    "# SHORTCUT TO SHRINK + CV2_IMSHOW\n",
    "def preview(image, width = 200):\n",
    "  image = image.copy()\n",
    "  return shrink(image, width = width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Cropping, Reflection Mask (`ref_th` edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image cropping\n",
    "def crop_img(img):\n",
    "  # Remove around 100 pixels from the start and end of the image in booth axes of the given (1080 x 1350) image\n",
    "  return img[100:1080 -100,100:1350-100]\n",
    "\n",
    "# remove wet reflections \n",
    "import cv2\n",
    "def get_reflections_mask(img_RGB, ref_th=0.875, expand_kernel=7):\n",
    "    # if img_gray.ndim == 3 and img_gray.shape[2] > 1:\n",
    "    #     img_gray = cv2.cvtColor(img_gray, cv2.COLOR_BGR2GRAY)\n",
    "    YUV = cv2.cvtColor(img_RGB, cv2.COLOR_RGB2YUV)\n",
    "    reflection_mask = (YUV[:,:,0] < ref_th * 255).astype('uint8')\n",
    "    reflection_mask = cv2.erode(reflection_mask,\n",
    "                                cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (expand_kernel, expand_kernel))).astype('uint8')\n",
    "    return reflection_mask.astype('bool')\n",
    "\n",
    "def inpaint_mask(rgb, mask):\n",
    "    inpainted = cv2.inpaint(rgb, mask.astype('uint8'), inpaintRadius=13, flags=cv2.INPAINT_TELEA)\n",
    "    return inpainted\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAHE contrast limited & other tests (edit)\n",
    "from documentation:   \n",
    "\"So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization.\n",
    "   \n",
    "\n",
    "contrast limiting separates our images into two phases, enabling us to further tune white vs darkspaces. Preliminary results (from test_clahe) don't look too good; **future directions should test clip limit and tile size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE DIFF BTWN REG HIST EQ AND CLAHE\n",
    "def test_equalizer(test_image, mode = 'compare'):\n",
    "  n_img = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
    "  eq = False\n",
    "  c = False\n",
    "  \n",
    "  print('original image:')\n",
    "  preview(n_img)\n",
    "\n",
    "  if mode == 'equalize' or mode == 'compare':\n",
    "    n_eq = cv.equalizeHist(n_img)\n",
    "    print('normal histogram equalization:')\n",
    "    preview(n_eq)\n",
    "\n",
    "  if mode == 'clahe' or mode == 'compare':\n",
    "    n_clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    n_cl = n_clahe.apply(n_img)\n",
    "    print('clahe: ')\n",
    "    preview(n_cl)\n",
    "  \n",
    "\n",
    "  # before eq: yellow, after: green\n",
    "  i = plt.hist(n_img.flat, alpha = 0.5, bins=100, range=(0, 255), color='orange')\n",
    "  j = plt.hist(n_eq.flat, alpha = 0.5, bins=100, range=(0, 255), color='r')\n",
    "  k = plt.hist(n_cl.flat, alpha = 0.5, bins=100, range=(0, 255), color='g') \n",
    "\n",
    "# test_equalizer(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PARAMETER TUNE\n",
    "clip_limit = 2.         # 2.0\n",
    "tile_size = (8, 8)    # (8,8)\n",
    "# -- PARAMETER TUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CLAHE uses from PARAMETER TUNING, visualizing hist, setting threshold values\n",
    "\n",
    "def test_clahe(image, tune = False, hist = False, threshold = False):\n",
    "  if tune: print('original bw image')\n",
    "  preview(image)\n",
    "  n_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # standard state clahe\n",
    "  n_clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "  n_cl = n_clahe.apply(n_img)\n",
    "\n",
    "  if tune: print('standard state CLAHE')\n",
    "\n",
    "  preview(n_cl)\n",
    "  \n",
    "  if tune:\n",
    "    # -- PARAMETER TUNE\n",
    "    clip_limit = 2.         # 2.0\n",
    "    tile_size = (8, 8)    # (8,8)\n",
    "    # -- PARAMETER TUNE\n",
    "    print('test parameters: \\n clip limit: ', clip_limit, '\\n tile size: ', tile_size)\n",
    "    n_clahe2 = cv.createCLAHE(clipLimit = clip_limit, \n",
    "                              tileGridSize = tile_size)\n",
    "\n",
    "    n_cl2 = n_clahe2.apply(n_img)\n",
    "    preview(n_cl2)\n",
    "\n",
    "  if hist:\n",
    "    # before eq: yellow, after: green\n",
    "    i = plt.hist(n_img.flat, alpha = 0.5, bins=100, range=(0, 255), color='y')\n",
    "    k = plt.hist(n_cl.flat, alpha = 0.5, bins=100, range=(0, 255), color='g') \n",
    "\n",
    "  if threshold:\n",
    "    print('attempt low, high, auto thresholding')\n",
    "    # lower thresh\n",
    "    i, thresh_low = cv2.threshold(n_cl, 150, 100, cv2.THRESH_BINARY)\n",
    "    preview(thresh_low)\n",
    "    # higher\n",
    "    j, thresh_high = cv2.threshold(n_cl, 150, 200, cv2.THRESH_BINARY_INV)\n",
    "    preview(thresh_high)\n",
    "    # find threshhold automatically\n",
    "    k, thresh_auto = cv2.threshold(n_cl, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    preview(thresh_auto)\n",
    "\n",
    "# test_clahe(images[7], tune = True, hist = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Equalization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images before histogram equalization w/ histogram plots\n",
    "\n",
    "def histogram_eq(prev_step, verbose = False):\n",
    "  i = 0\n",
    "  \n",
    "  # Conduct histogram equalization\n",
    "  equalized = []\n",
    "  for image in prev_step:\n",
    "    image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # or convert\n",
    "    equ = cv.equalizeHist(image_bw)\n",
    "    equalized.append(equ)\n",
    "    res = np.hstack((image_bw,equ)) # stacking images side-by-side\n",
    "    if verbose: preview(res)\n",
    "\n",
    "  if verbose: \n",
    "    for image in prev_step:\n",
    "      hist, bins = np.histogram(image[0].flatten(),256,[0,256])\n",
    "      cdf = hist.cumsum()\n",
    "      cdf_normalized = cdf * hist.max()/ cdf.max()\n",
    "\n",
    "      print(\"Image\" , i)\n",
    "      preview(image)\n",
    "      plt.plot(cdf_normalized, color = 'b')\n",
    "      plt.hist(image.flatten(),256,[0,256], color = 'r')\n",
    "      plt.xlim([0,256])\n",
    "      plt.legend(('cdf','histogram'), loc = 'upper left')\n",
    "      plt.show()\n",
    "      i+=1\n",
    "    \n",
    "  return equalized \n",
    "\n",
    "# histogram_eq(images, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying Contrast Limited Adaptive Histogram Equalization (CLAHE)- histogram \n",
    "# equalization in specific tile regions to reduced information loss\n",
    "\n",
    "def clahe(no_ref_images, verbose = True, show_all = False):\n",
    "  clahe_images = []\n",
    "  stacked1 = []\n",
    "  for img in no_ref_images:\n",
    "    if verbose : preview(img)\n",
    "    image = cv.cvtColor(img, cv2.COLOR_BGR2GRAY) # or convert \n",
    "    # SET PARAMETERS\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size) # createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl1 = clahe.apply(image)\n",
    "    if verbose: preview(cl1)\n",
    "    clahe_images.append(cl1)\n",
    "    res = np.hstack((image, cl1)) #stacking images side-by-side\n",
    "    stacked1.append(res)\n",
    "    if verbose: print('--')\n",
    "\n",
    "  if show_all:\n",
    "    equalized = histogram_eq(no_ref_images)\n",
    "    for i in range(len(stacked1)):\n",
    "      res = np.hstack((stacked1[i], equalized[i]))\n",
    "      cv2.putText(res, \"Original\", (20, 35),\n",
    "        cv.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 3);\n",
    "      cv2.putText(res, \"CLAHE\", (1200, 35),\n",
    "        cv.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 3);\n",
    "      cv2.putText(res, \"equalized\", (2350, 35),\n",
    "        cv.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 3);\n",
    "      preview(res, width = 800)\n",
    "\n",
    "  return clahe_images\n",
    "\n",
    "clahe_images = clahe(images, show_all=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
